from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier

xgb = XGBClassifier(
    objective="binary:logistic",
    eval_metric="logloss",
    n_jobs=-1,
    random_state=42
)

param_dist = {
    "n_estimators": [200, 400, 600],
    "learning_rate": [0.03, 0.05, 0.1],
    "max_depth": [3, 4, 5],
    "subsample": [0.7, 0.9],
    "colsample_bytree": [0.7, 0.9],
    "min_child_weight": [1, 5, 10],
}

search = RandomizedSearchCV(
    xgb,
    param_distributions=param_dist,
    n_iter=15,
    scoring="roc_auc",
    cv=3,
    n_jobs=-1,
    verbose=1
)

search.fit(X, y)
print(search.best_params_, search.best_score_)

# First I ran this part of the code, then I used the best parameters in this
# catboost model

# 1. Load data ---------------------------------------------------------

train = pd.read_csv("aluminum_coldRoll_train.csv")
test  = pd.read_csv("aluminum_coldRoll_testNoY.csv")

# Save test IDs for submission
test_ids = test["ID"]

# Drop ID from features
train = train.drop(columns=["ID"])
test_X = test.drop(columns=["ID"])

# Split into X, y
X = train.drop(columns=["y_passXtremeDurability"])
y = train["y_passXtremeDurability"]   # 0/1 integers

# 2. Set up preprocessing (one-hot for categoricals) -------------------

categorical_cols = [
    "alloy", "cutTemp", "rollTemp",
    "topEdgeMicroChipping", "blockSource", "machineRestart"
]

numeric_cols = [
    "firstPassRollPressure",
    "secondPassRollPressure",
    "contourDefNdx",
    "clearPassNdx"
]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ("num", "passthrough", numeric_cols),
    ]
)

# 3. Define XGBoost model ----------------------------------------------

xgb_clf = XGBClassifier(
    n_estimators=600,
    learning_rate=0.05,
    max_depth=3,
    subsample=0.9,
    colsample_bytree=0.9,
    objective="binary:logistic",
    eval_metric="logloss",
    random_state=42,
    n_jobs=-1
)

# 4. Build pipeline: preprocessing + model -----------------------------

model = Pipeline(steps=[
    ("preprocess", preprocess),
    ("xgb", xgb_clf)
])

# 5. Cross-validated ROC AUC on training data --------------------------

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_roc = cross_val_score(model, X, y, cv=cv, scoring="roc_auc", n_jobs=-1)

print(f"XGBoost CV ROC AUC: {cv_roc.mean():.4f} Â± {cv_roc.std():.4f}")

# 6. Fit final model on full training set ------------------------------

model.fit(X, y)

# 7. Predict probabilities on Kaggle test set --------------------------

test_proba = model.predict_proba(test_X)[:, 1]  # probability of class 1 (success)

# keep strictly inside (0,1) if you want to be extra safe
eps = 1e-6
test_proba = np.clip(test_proba, eps, 1 - eps)

# 8. Build submission file ---------------------------------------------

submission = pd.DataFrame({
    "ID": test_ids,
    "y_passXtremeDurability": test_proba
})

submission.to_csv("submission_xgb.csv", index=False)
print("Saved submission_xgb.csv")
